{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d5436c6",
      "metadata": {
        "id": "2d5436c6"
      },
      "source": [
        "# Final Exam Project Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dee6af5",
      "metadata": {
        "id": "2dee6af5"
      },
      "source": [
        "Create a python program that will compute the text document similarity between different documents. Your implementation will take a list of documents as an input text corpus, and it will compute\n",
        "a dictionary of words for the given corpus. Later, when a new document (i.e, search document) is\n",
        "provided, your implementation should provide a list of documents that are similar to the given search\n",
        "document, in descending order of their similarity with the search document.\n",
        "For computing similarity between any two documents in our question, you can use the following\n",
        "distance measures (optionally, you can also use any other measure as well).\n",
        "\n",
        "1. dot product between the two vectors\n",
        "2. distance norm (or Euclidean distance) between two vectors .e.g. || u âˆ’ v ||\n",
        "\n",
        "As part of answering the question, you can also compare and comment on which of the two\n",
        "methods (or any other measure if you have used some other measure) will perform better and what\n",
        "are the reasons for it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54152f3f",
      "metadata": {
        "id": "54152f3f"
      },
      "source": [
        "##### Relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d671f722",
      "metadata": {
        "id": "d671f722"
      },
      "outputs": [],
      "source": [
        "# import relevant libraries \n",
        "\n",
        "from numpy import dot #  to calculate dot-product\n",
        "from numpy.linalg import norm # to calculate norm of vector\n",
        "import string # for punctuation removal\n",
        "import pandas as pd # to create dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d09ff4a0",
      "metadata": {
        "id": "d09ff4a0"
      },
      "source": [
        "##### Class text_document_similarity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78db5c50",
      "metadata": {
        "id": "78db5c50"
      },
      "outputs": [],
      "source": [
        "class text_document_similarity():\n",
        "    '''\n",
        "    This class contains the functions document_term_matrix() and text_by_distance()\n",
        "    '''\n",
        "\n",
        "    def document_term_matrix(text_corpus: list):\n",
        "        ''' \n",
        "        Function creates a document-term-matrix out of a list of documents as a pandas dataframe\n",
        "\n",
        "        Input: \n",
        "        Text corpus: list of documents, i.e., list of strings (it is also possible just to add one text as only a document-term-matrix is created)\n",
        "\n",
        "        Preprocessing of text input includes: \n",
        "        Standardization to lowercase characters, punctuation removal\n",
        "        Stopword removal and stemming, for example, not included as NLP libraries have to be used \n",
        "        \n",
        "        Tokenization:\n",
        "        Tokenize all words and count them how many times they appear in each document\n",
        "\n",
        "        Document-term-matrix: \n",
        "        Words as columns and the different docs as rows\n",
        "        The values are counted appearances of each word in that specific document\n",
        "        \n",
        "        Output:\n",
        "        Document-term-matrix as pandas dataframe\n",
        "        '''\n",
        "        # check first that input is correct\n",
        "        try:\n",
        "            assert isinstance(text_corpus, list), \"text_corpus parameter: Input must be a list.\"\n",
        "            assert all(isinstance(i, str) for i in text_corpus), \"text_corpus parameter: Not all elements in the list are strings.\"\n",
        "\n",
        "            # preprocessing of text input\n",
        "            # putting processed text input into new variable text_proc\n",
        "            # not considered: stopword removal, stemming etc. as NLP libraries have to be used\n",
        "            text_proc = text_corpus.copy()\n",
        "\n",
        "            # lower case of characters\n",
        "            for i in range(len(text_proc)):\n",
        "                text_proc[i] = text_proc[i].lower()\n",
        "\n",
        "            # punctuation removal \n",
        "            for i in range(len(text_proc)):\n",
        "                text_proc[i] = text_proc[i].translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "            # tokenize text input, only single words are considered\n",
        "            tokens = []\n",
        "            for i in range(len(text_proc)):\n",
        "                sub_tokens = text_proc[i].split() # split each text of text input by spaces and add it to a list\n",
        "                for j in range(len(sub_tokens)): \n",
        "                    tokens.append(sub_tokens[j]) # append all identified tokens to the final token list\n",
        "\n",
        "            tokens_unique = [] \n",
        "            # consideration of duplicates in the token list, thus, removing duplicates\n",
        "            [tokens_unique.append(x) for x in tokens if x not in tokens_unique]\n",
        "\n",
        "            # creation of document-term-matrix (DTM): \n",
        "            # tokens are our columns, we count the appearance of these tokens in each document \n",
        "            dtm = []\n",
        "            for i in range(len(text_proc)):\n",
        "                sub_text = text_proc[i].split() # take each individual text input as a list of their contained words\n",
        "                sub_dtm = [] # temporary variable which will be added to dtm\n",
        "                for j in range(len(tokens_unique)): # go through each token once\n",
        "                    count = sub_text.count(tokens_unique[j]) # count the word appearances of tokens in one individual text input\n",
        "                    sub_dtm.append(count) # add the appearance to the sub_dtm\n",
        "                dtm.append(sub_dtm) # final dtm by adding all sub_dtm in one list\n",
        "\n",
        "            # converting the dtm to a pandas dataframe\n",
        "            dtm = pd.DataFrame(dtm)\n",
        "\n",
        "            # adding the column names which are the tokens\n",
        "            dtm.columns = tokens_unique\n",
        "\n",
        "            # return the document-term-matrix \n",
        "            return dtm\n",
        "\n",
        "        except AssertionError as msg:\n",
        "                print(msg)\n",
        "\n",
        "                \n",
        "    def text_by_distance(new_text: list, text_corpus: list, distance_measure=\"cosine\"):\n",
        "        '''\n",
        "        Function that returns original text corpus in a sequence depending on the distance to a search document\n",
        "\n",
        "        Input:\n",
        "          new_text: New text document, i.e., list with one string\n",
        "          text_corpus: Text corpus (initial text input), i.e., list of strings (for practical reasons this list needs more than 1 text, otherwise this function makes no sense)\n",
        "          distance_measure: Distance measure, i.e. user can choose between cosine similarity or euclidean distance, cosine similarity is default\n",
        "        To choose cosine similarity type \"cosine\", to choose Euclidean distance type \"Euclidean\"\n",
        "        Cosine similarity, higher values are better (more similar)\n",
        "        Euclidean, smaller values are better (more similar)\n",
        "\n",
        "        Output:\n",
        "          list_of_text: The sequence depends on the distance to the new text input\n",
        "        Smallest distance first, maximal distance last (descending order), i.e., more similar documents first\n",
        "        Before each document a ranking is added, for example, 1 means most similar to search document\n",
        "        '''\n",
        "        # check that input is correct\n",
        "        try:\n",
        "            assert isinstance(new_text, list), \"new_text parameter: Input must be a list.\"\n",
        "            assert len(new_text)==1, \"new_text parameter: Only one string in the list is allowed.\"\n",
        "            assert isinstance(new_text[0], str), \"new_text parameter: Element in list must be a string.\"\n",
        "            \n",
        "            assert distance_measure in (\"cosine\",\"euclidean\"), \"Check distance measure input.\"\n",
        "            \n",
        "            assert isinstance(text_corpus, list), \"text_corpus parameter: Input must be a list.\"\n",
        "            assert len(text_corpus)>1, \"text_corpus parameter: More than one string should be used as input.\"\n",
        "            assert all(isinstance(i, str) for i in text_corpus), \"text_corpus parameter: Not all elements in the list are strings.\"\n",
        "\n",
        "            # same processes to create the document-term-matrix\n",
        "            dtm2 = text_document_similarity.document_term_matrix(new_text) \n",
        "            \n",
        "            # get original text corpus dtm\n",
        "            dtm = text_document_similarity.document_term_matrix(text_corpus)\n",
        "\n",
        "            # get the vector of the new text input in case you would have the columns of the dtm of the original input text corpus\n",
        "            vector_new_text = []\n",
        "            for column in dtm:\n",
        "                if column in dtm2.columns:\n",
        "                    vector_new_text.append(dtm2[column][0])\n",
        "                else:\n",
        "                    vector_new_text.append(0) \n",
        "\n",
        "            # create a dictionary with two columns: doc and distance to new text input\n",
        "            doc_dic = {}\n",
        "\n",
        "            # distance measure cosine similarity\n",
        "            if distance_measure==\"cosine\": \n",
        "                for i in range(dtm.shape[0]):\n",
        "                    # for loop going through each document and calculating the distance to new text input\n",
        "                    # putting doc as key and distance as value\n",
        "                    a = dtm.iloc[i] # looping through all documents, one document per loop\n",
        "                    b = vector_new_text # comparing distance to new search document\n",
        "                    if (norm(a)==0) or (norm(b)==0): # if norm of a or b is 0, take over 0 as cosine similarity (lowest possible similarity) as you cannot divide by 0\n",
        "                    # norm could be 0, for example, when tokens of search document are not available in orginal text corpus\n",
        "                      cosine = 0\n",
        "                    else:\n",
        "                      cosine = dot(a,b)/(norm(a)*norm(b))\n",
        "                    doc_dic[i] = cosine\n",
        "\n",
        "            # distance measure Euclidean distance\n",
        "            if distance_measure==\"euclidean\":\n",
        "                for i in range(dtm.shape[0]): \n",
        "                    # for loop going through each document and calculating the distance to new text input\n",
        "                    # putting doc as key and distance as value\n",
        "                    a = dtm.iloc[i]\n",
        "                    b = vector_new_text\n",
        "                    dist = 0\n",
        "                    for j in range(dtm.shape[1]): \n",
        "                        dist += (b[j] - a[j]) ** 2\n",
        "                    dist = dist ** 0.5\n",
        "                    doc_dic[i] = dist # add doc as key and dist as value\n",
        "\n",
        "            # data frame with original text corpus index as index and dist to new text input as column\n",
        "            docs_sorted_by_dist = pd.DataFrame.from_dict(doc_dic, orient=\"index\", columns=[\"dist\"])\n",
        "\n",
        "            if distance_measure==\"cosine\": # sort from high to low distance\n",
        "                docs_sorted_by_dist.sort_values(\"dist\", ascending=False, inplace=True)\n",
        "\n",
        "            if distance_measure==\"euclidean\": # sort from low to high distance\n",
        "                docs_sorted_by_dist.sort_values(\"dist\", ascending=True, inplace=True)\n",
        "\n",
        "            # get original text as output by accessing the original text corpus list in the right sequence\n",
        "            docs_list = docs_sorted_by_dist.index.tolist()\n",
        "            list_of_text = []\n",
        "            for i in range(len(docs_sorted_by_dist.index)):\n",
        "                # for loop accessing the original text corpus list in right sequence according to distance with new text input\n",
        "                x = text_corpus[docs_list[i]]\n",
        "                # adding also ranking, distance with used measurement rounded to 2 decimal places and the text \n",
        "                list_of_text.append([\"Ranking: \" + str(i+1) + \", Distance (\" + str(distance_measure) + \"): \" + str(docs_sorted_by_dist[\"dist\"].iloc[i].round(2)) + \", Text: \" + x])\n",
        "\n",
        "            print(\"This is the text which is compared to the text corpus: \", new_text, \"\\n\")\n",
        "            return list_of_text\n",
        "        \n",
        "        except AssertionError as msg:\n",
        "                print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c6c1afe",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6c1afe",
        "outputId": "95f2493a-3bed-4707-f3ac-6b63abe0f16e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the text which is compared to the text corpus:  ['Are you beautiful?'] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Ranking: 1, Distance (cosine): 0.67, Text: Dogs are beautiful'],\n",
              " ['Ranking: 2, Distance (cosine): 0.29, Text: Cool! You made it'],\n",
              " ['Ranking: 3, Distance (cosine): 0.0, Text: Hello, what a nice day']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# test with text input \"cosine\"\n",
        "text_corpus = [\"Dogs are beautiful\", \"Hello, what a nice day\", \"Cool! You made it\"]\n",
        "new_text = [\"Are you beautiful?\"]\n",
        "\n",
        "text_document_similarity.text_by_distance(new_text, text_corpus, \"cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "335373a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "335373a6",
        "outputId": "0b65111c-5a18-4720-e836-e5c7eb6cd909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the text which is compared to the text corpus:  ['That is not bad'] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Ranking: 1, Distance (euclidean): 2.0, Text: I am great'],\n",
              " [\"Ranking: 2, Distance (euclidean): 2.0, Text: They're very lazy\"],\n",
              " ['Ranking: 3, Distance (euclidean): 2.45, Text: This is very very amazing']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# test with text input \"euclidean\"\n",
        "text_corpus1 = [\"I am great\", \"They're very lazy\", \"This is very very amazing\"]\n",
        "new_text1 = [\"That is not bad\"]\n",
        "\n",
        "text_document_similarity.text_by_distance(new_text1, text_corpus1, \"euclidean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5aa96d1b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aa96d1b",
        "outputId": "1833a47f-fcd5-4728-f1f1-a1e77368e82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the text which is compared to the text corpus:  ['Are you beautiful?'] \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Ranking: 1, Distance (cosine): 0.67, Text: Dogs are beautiful'],\n",
              " ['Ranking: 2, Distance (cosine): 0.29, Text: Cool! You made it'],\n",
              " ['Ranking: 3, Distance (cosine): 0.0, Text: Hello, what a nice day']]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# test with text input \"cosine\" (default)\n",
        "text_corpus2 = [\"Dogs are beautiful\", \"Hello, what a nice day\", \"Cool! You made it\"]\n",
        "new_text2 = [\"Are you beautiful?\"]\n",
        "\n",
        "text_document_similarity.text_by_distance(new_text2, text_corpus2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}